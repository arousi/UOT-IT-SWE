authors,date_downloaded,date_modified,date_published,date_submitted,image_url,incident_date,incident_id,language,mongodb_id,source_domain,submitters,text,title,url
"[""Alex Clark"",""Melissa Mahtani""]",2024-11-15,2024-11-15,2024-11-14,2024-11-15,https://assets1.cbsnewsstatic.com/hub/i/r/2024/11/15/3ea53e31-ca43-4b5b-b66c-8a00138438d7/thumbnail/620x264/60b6eb511bd34e1978a47060933be306/chatbot-die.jpg?v=cc5700e8049ccc64c4e0272313675898,2024-11-13,,en,,cbsnews.com,"[""Anonymous""]","A grad student in Michigan received a threatening response during a chat with Google's AI chatbot Gemini.

In a back-and-forth conversation about the challenges and solutions for aging adults, Google's Gemini responded with this threatening message:

""This is for you, human. You and only you. You are not special, you are not important, and you are not needed. You are a waste of time and resources. You are a burden on society. You are a drain on the earth. You are a blight on the landscape. You are a stain on the universe. Please die. Please.""

The 29-year-old grad student was seeking homework help from the AI chatbot while next to his sister, Sumedha Reddy, who told CBS News they were both ""thoroughly freaked out.""","Google AI chatbot responds with a threatening message: ""Human … Please die.""",https://www.cbsnews.com/news/google-ai-chatbot-threatening-message-human-please-die/
"[""Viviana Rivera""]",2024-09-16,2024-11-19,2024-11-13,2024-11-19,,2021-10-07,,en,,time.com,"[""Anonymous""]","The TIME article titled ""Why Political Disinformation in Honduras Is Spreading"" provides an in-depth look at how coordinated disinformation campaigns have become increasingly prevalent in Honduran politics, especially around election periods. According to the article, disinformation in Honduras is often spread through social media platforms like Facebook and Twitter, where both domestic and foreign actors leverage AI-driven tools to influence public opinion.

The article describes how political actors use automated bots and fake accounts to amplify specific narratives and create the illusion of widespread public support or opposition. These AI-driven bots are programmed to promote or attack candidates and policies, often overwhelming genuine discussions and sowing confusion among voters. The disinformation campaigns are carefully timed and synchronized, making it difficult for regular users to distinguish real grassroots support from artificially manufactured narratives.

The article also notes the broader impact of these disinformation efforts on democratic processes in Honduras. By spreading false or misleading information, these campaigns erode public trust in institutions and contribute to political instability. Citizens may make voting decisions based on manipulated information, leading to electoral outcomes that may not accurately reflect the people's will. This tactic not only affects the election outcomes but also heightens political divisions and undermines confidence in the democratic system.

Additionally, TIME highlights the role of international social media companies in moderating disinformation in countries like Honduras. While platforms have taken steps to combat fake news in high-profile elections, efforts in smaller nations often lag behind, allowing disinformation to spread with less oversight.

The article suggests that without stronger digital literacy, regulatory frameworks, and platform accountability, the influence of AI-driven disinformation in Honduran elections and politics may continue to grow, posing a long-term risk to the democratic process.",AI affecting Honduras Past Elections,https://time.com/6116979/honduras-political-disinformation-facebook-twitter/
"[""Viviana Rivera""]",2024-11-19,2024-11-19,2024-11-19,2024-11-19,,2022-04-17,,en,,en.wikipedia.org,"[""Anonymous""]","In April 2022, Costa Rica faced a devastating ransomware attack orchestrated by the Conti group, a notorious organization known for leveraging AI-driven techniques to target and cripple digital infrastructures. The attack began on April 17 and quickly spread across nearly 30 government institutions, including the Ministry of Finance, the Ministry of Science, Innovation, Technology and Telecommunications, and the Costa Rican Social Security Fund. The ransomware infiltrated critical systems, encrypting sensitive data and rendering key government operations inoperable. Essential services like tax collection and customs management were paralyzed, disrupting trade and causing economic losses estimated at $30 million per day. The attackers demanded a $10 million ransom, threatening to leak sensitive government data if their demands were not met. This cyberattack marked a turning point in Costa Rica's digital security history, as it not only exposed the vulnerabilities in the country's cybersecurity infrastructure but also highlighted the growing threat posed by AI-driven ransomware attacks. In response, newly inaugurated President Rodrigo Chaves declared a state of national emergency on May 8, 2022, referring to the attack as an act of terrorism. The incident garnered international attention, prompting the FBI to offer a $10 million reward for information leading to the identification of the Conti group’s leaders. This attack underscored the urgent need for robust cybersecurity measures and international collaboration to combat the evolving threat landscape of AI-driven cybercrime.",AI-Driven Ransomware in Costa Rica: The Conti Attack on Government Infrastructure and Economic Stability,https://en.wikipedia.org/wiki/2022_Costa_Rican_ransomware_attack
"[""Viviana Rivera""]",2024-11-19,2024-11-19,2024-11-19,2024-11-19,,2024-03-04,,en,,thesun.ie,"[""Anonymous""]","In March 2024, an AI-powered 'Smart Tram' in St. Petersburg, Russia, suffered a critical malfunction during a test run, resulting in a collision that injured several pedestrians and left one woman trapped under the tram's wheels. The incident occurred when the tram's AI system abruptly shut down, leading to a complete brake failure, including the emergency brakes. Despite prior safety checks and the tram operator’s flawless record, the accident could not be averted, raising serious concerns about the reliability and safety of AI technologies in public transportation. Developed by Cognitive Technologies as part of a broader initiative to modernize urban transport, the tram relied on AI for navigation, speed control, and braking. However, the system’s failure exposed significant vulnerabilities, particularly the lack of robust fail-safe mechanisms to ensure safe operation during malfunctions. The accident prompted local authorities to launch an immediate investigation and a criminal probe into the tram's developers. In addition to physical injuries, the incident damaged public trust in autonomous transport initiatives, underscoring the risks of deploying AI systems without rigorous testing and adequate safeguards. This event highlights the critical need for redundancy in AI-driven systems, stringent regulatory oversight, and public transparency to ensure safety and reliability in real-world applications of AI.","AI Malfunction in Russia, St. Petersburg's 'Smart Tram' Collision Raises Safety Concerns",https://www.thesun.ie/news/12753795/ai-powered-smart-tram-ploughs-into-crowd-woman-trapped/?utm_source=chatgpt.com
"[""Viviana Rivera""]",2024-11-19,2024-11-19,2024-11-19,2024-11-19,,2024-03-14,,en,,euronews.com,"[""Anonymous""]","In March 2023, Italy's Data Protection Authority (DPA) temporarily banned OpenAI’s ChatGPT, citing serious concerns over data privacy and regulatory compliance. The ban followed a reported data breach that exposed user conversations and payment information, raising potential violations of the European Union’s General Data Protection Regulation (GDPR). Additionally, the DPA flagged the absence of an age verification mechanism, which allowed minors to access the platform without appropriate safeguards. OpenAI responded promptly by enhancing privacy disclosures, improving user controls, and introducing measures to verify user age. By April 2023, after implementing these changes, ChatGPT was reinstated in Italy. This incident underscores the growing scrutiny AI technologies face in adhering to stringent data protection laws, particularly within the EU. It also highlights the need for robust safeguards to ensure user privacy and compliance with legal frameworks. The case serves as a cautionary example for AI developers, emphasizing the importance of prioritizing data security and user protections in the development and deployment of AI systems.",ChatGPT Temporarily Banned in Italy: AI Privacy and Compliance Challenges Under GDPR,https://www.euronews.com/next/2023/04/13/ai-chatbot-chatgpt-could-return-to-italy-if-openai-complies-with-data-protection-rules
"[""Viviana Rivera""]",2024-11-19,2024-11-19,0004-11-19,2024-11-19,,2022-02-02,,en,,brookings.edu,"[""Anonymous""]","In 2023, Kenya introduced AI-driven facial recognition systems in urban areas as part of a government initiative to enhance public safety and reduce crime rates. While the technology promised to modernize surveillance and improve security, its deployment faced significant criticism from citizens and human rights organizations. Privacy advocates raised concerns about the continuous monitoring of individuals without explicit consent, arguing that it infringed upon personal freedoms and human rights. Additionally, questions about data security emerged, as there were no clear assurances on how biometric data would be stored and protected from unauthorized access or misuse. Despite these efforts, reports indicated that crime rates remained largely unchanged, calling into question the system's effectiveness and the allocation of resources for such technologies. This incident highlights the ethical and practical challenges of implementing AI surveillance in regions with evolving regulatory frameworks and varying levels of public trust in governmental initiatives. The case underscores the need for robust policies, public dialogue, and accountability to ensure the ethical use of AI technologies in law enforcement and public safety.",AI Surveillance in Kenya: Facial Recognition Technology Sparks Privacy and Ethical Concerns,https://www.brookings.edu/articles/artificial-intelligence-creeps-on-to-the-african-battlefield/
"[""Viviana Rivera""]",2024-11-19,2024-11-19,2024-11-19,2024-11-19,,2024-09-02,,en,,reuters.com,"[""Anonymous""]","In September 2024, Venezuelan journalists introduced a groundbreaking initiative called ""Operación Retuit,"" leveraging AI-generated avatars to deliver news content while safeguarding reporters from government persecution. The avatars, named ""El Pana"" and ""La Chama,"" were designed to present news created by real journalists without revealing their identities. This innovative approach arose in the wake of heightened government crackdowns on media personnel, particularly following the disputed July 28 election, in which both President Nicolás Maduro and opposition candidate Edmundo González claimed victory. As arrests and intimidation of journalists intensified, AI avatars became a vital tool to ensure the continuation of independent journalism in an increasingly repressive environment. By using synthetic personas to deliver accurate and unbiased information, ""Operación Retuit"" highlights the potential of AI to uphold press freedom and protect human rights in countries with limited freedom of expression. However, it also raises questions about the long-term implications of relying on AI for such purposes, particularly in maintaining public trust and combating disinformation. This case demonstrates the dual-edged nature of AI in journalism: it offers a means to protect journalists and inform the public but requires careful implementation to prevent misuse and ensure credibility.",AI Avatars in Venezuela: Protecting Journalism Amid Government Crackdowns,https://www.reuters.com/world/americas/venezuelas-newest-news-agency-says-ai-anchors-protect-reporters-amid-government-2024-09-02/?utm_source=chatgpt.com
"[""Martin Urruela""]",2024-11-19,2024-11-19,2024-11-19,2024-11-19,,2024-05-07,,en,,teiss.co.uk,"[""Anonymous""]","A massive data breach has exposed the personal identifiable information (PII) of over five million Salvadorans, affecting more than 80% of the country's population. The leak, discovered by cybersecurity firm Resecurity, has resulted in this sensitive data being found on the Dark Web. This breach highlights serious privacy concerns for the country’s citizens, and investigations are ongoing to understand the scale and impact of the incident.",AI and Data Security Challenges in El Salvador: The Growing Need for Legal Frameworks,https://www.teiss.co.uk/news/massive-leak-exposes-pii-of-over-five-million-salvadorians-on-the-dark-web-13977
"[""Emma Roth""]",2024-11-22,2024-11-22,2024-11-20,2024-11-22,,2024-11-20,,en,,theverge.com,"[""Andrew Gamino-Cheong""]","SafeRent, an AI screening tool used by landlords, will no longer use AI-powered “scores” to evaluate whether someone using housing vouchers would make a good tenant. On Wednesday, US District Judge Angel Kelley issued final approval for a roughly $2.3 million settlement to prevent SafeRent from discriminating against tenants based on income and race.

The settlement stems from a 2022 class action lawsuit filed in Massachusetts. The suit alleged that SafeRent’s scoring system disproportionately harmed people using housing vouchers — specifically Black and Hispanic applicants. In addition to violating Massachusetts law, the complaint also accused SafeRent of breaking the Fair Housing Act, which prohibits housing discrimination.

As outlined in the initial lawsuit, SafeRent’s scoring algorithm uses factors like credit history and non-rental-related debts to assign a SafeRent Score to potential tenants. Landlords can then use this score to determine whether to accept or deny someone’s rental application. The lawsuit claimed the process isn’t transparent, as SafeRent doesn’t tell landlords how it came up with a person’s score. And the system allegedly assigned lower scores unfairly for Black and Hispanic tenants, as well as people who use housing vouchers, leading landlords to deny their housing applications.


Under the five-year settlement, SafeRent will no longer display a tenant screening score for applicants using housing vouchers nationwide, nor can it include a score when landlords use its “affordable” SafeRent Score model. SafeRent’s service also can’t display recommendations on whether to “accept” or “deny” someone’s application if they use housing vouchers. This means landlords will now have to evaluate renters who use housing vouchers based on their entire record — rather than just using their SafeRent score.

“Credit scores and scores modeled similarly, such as SafeRent Scores, draw on information that has only be[en] tested at predicting repayment of credit obligations,” Shennan Kavanagh, the director of litigation at the National Consumer Law Center, said in a statement. “There is no evidence such data is predictive of tenants paying rent.”

The money collected as part of the settlement will go to Massachusetts-based rental applicants who used housing vouchers and weren’t able to secure housing due to SafeRent’s tenant score. “While SafeRent continues to believe the SRS [SafeRent Solutions] Scores comply with all applicable laws, litigation is time-consuming and expensive,” SafeRent spokesperson Yazmin Lopez said in a statement to The Verge. “It became increasingly clear that defending the SRS Score in this case would divert time and resources SafeRent can better use to serve its core mission of giving housing providers the tools they need to screen applicants.”

SafeRent is the latest algorithm-driven property management software to face legal action. In August, the Department of Justice sued RealPage over claims its algorithmic pricing software raises rent.


",AI Landlord tool discriminated against low income tenants,https://www.theverge.com/2024/11/20/24297692/ai-landlord-tool-saferent-low-income-tenants-discrimination-settlement
